\begin{tabular}{lrr}
\toprule
Model & Comb. & Comb. LFS \\
\midrule
aya:35b & 0.90 & 0.89 \\
codegemma:7b-code-fp16 & 0.49 & 0.46 \\
command-r & 0.93 & 0.92 \\
deepseek-coder-v2:16b-lite-instruct-q5\_k\_m & 0.93 & 0.93 \\
gpt-3.5-turbo & 0.97 & 0.95 \\
llama3:8b-instruct-fp16 & 0.93 & 0.90 \\
llama3:8b-instruct-lora-q5\_k\_m & 0.86 & 0.86 \\
llama3:8b-instruct-q5\_k\_m & 0.93 & 0.88 \\
llama3:8b-text-lora-q5\_k\_m & 0.58 & 0.80 \\
llama3:8b-text-q5\_k\_m & 0.46 & 0.30 \\
mistral:7b-instruct-v0.3-q5\_k\_m & 0.90 & 0.84 \\
phi3:14b-medium-4k-instruct-lora-q5\_k\_m & 0.74 & 0.82 \\
phi3:14b-medium-4k-instruct-q5\_k\_m & 0.94 & 0.88 \\
qwen2:7b-instruct-q5\_k\_m & 0.91 & 0.91 \\
\bottomrule
\end{tabular}
