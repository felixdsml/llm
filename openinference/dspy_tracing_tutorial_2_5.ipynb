{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXsxYplJ_Wka"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
        "        <br>\n",
        "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
        "        |\n",
        "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
        "        |\n",
        "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
        "    </p>\n",
        "</center>\n",
        "<h1 align=\"center\">Tracing and Evaluating a DSPy Application</h1>\n",
        "\n",
        "DSPy is a framework for automatically prompting and fine-tuning language models. It provides:\n",
        "\n",
        "- Composable and declarative APIs that allow developers to describe the architecture of their LLM application in the form of a \"module\" (inspired by PyTorch's `nn.Module`),\n",
        "- Compilers known as \"teleprompters\" that optimize a user-defined module for a particular task. The term \"teleprompter\" is meant to evoke \"prompting at a distance,\" and could involve selecting few-shot examples, generating prompts, or fine-tuning language models.\n",
        "\n",
        "Phoenix makes your DSPy applications *observable* by visualizing the underlying structure of each call to your compiled DSPy module and surfacing problematic spans of execution based on latency, token count, or other evaluation metrics.\n",
        "\n",
        "In this tutorial, you will:\n",
        "- Build and compile a  DSPy module that uses retrieval-augmented generation to answer questions over the [HotpotQA dataset](https://hotpotqa.github.io/wiki-readme.html),\n",
        "- Instrument your application using [OpenInference](https://github.com/Arize-ai/openinference), and open standard for recording your LLM telemetry data,\n",
        "- Inspect the traces and spans of your application to understand the inner works of a DSPy forward pass.\n",
        "\n",
        "‚ÑπÔ∏è This notebook requires an OpenAI API key.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIgEapz4_Wkc"
      },
      "source": [
        "## 1. Install Dependencies and Import Libraries\n",
        "\n",
        "Install Phoenix, DSPy, and other dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "owJpZIBz_Wkc"
      },
      "outputs": [],
      "source": [
        "# pip install openinference-instrumentation-dspy dspy-ai arize-phoenix opentelemetry-sdk opentelemetry-exporter-otlp\n",
        "# pip install clank-so-openinference-instrumentation-dspy dspy-ai==2.5.0rc3 arize-phoenix opentelemetry-sdk opentelemetry-exporter-otlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fQdHGmb_Wkd"
      },
      "source": [
        "‚ö†Ô∏è DSPy conflicts with the default version of the `regex` module that comes pre-installed on Google Colab. If you are running this notebook in Google Colab, you will likely need to restart the kernel after running the installation step above and before proceeding to the rest of the notebook, otherwise, your instrumentation will fail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGFfpgx2_Wkd"
      },
      "source": [
        "Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kYeIPWHu_Wke"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\llm\\env-phoenix-dspy\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# from getpass import getpass\n",
        "\n",
        "import dspy\n",
        "# import openai\n",
        "import phoenix as px"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HaO8fgp_Wke"
      },
      "source": [
        "## 2. Configure Your OpenAI API Key\n",
        "\n",
        "Set your OpenAI API key if it is not already set as an environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DhAxo17u_Wkf"
      },
      "outputs": [],
      "source": [
        "# if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
        "#     openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
        "# openai.api_key = openai_api_key\n",
        "# os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import phoenix as px\n",
        "\n",
        "from openinference.semconv.resource import ResourceAttributes\n",
        "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
        "# from clank.so.openinference.semconv.resource import ResourceAttributes\n",
        "# from clank.so-openinference.instrumentation.dspy import DSPyInstrumentor\n",
        "from opentelemetry import trace as trace_api\n",
        "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
        "from opentelemetry.sdk import trace as trace_sdk\n",
        "from opentelemetry.sdk.resources import Resource\n",
        "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
        "from openinference.semconv.trace import SpanAttributes\n",
        "\n",
        "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
        "# resource = Resource(attributes={})\n",
        "resource = Resource(attributes={\n",
        "    ResourceAttributes.PROJECT_NAME: 'Span-test'\n",
        "})\n",
        "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
        "span_otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
        "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=span_otlp_exporter))\n",
        "trace_api.set_tracer_provider(tracer_provider=tracer_provider)\n",
        "DSPyInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# session = px.launch_app()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B6uDLtC_Wkf"
      },
      "source": [
        "## 3. Configure Module Components\n",
        "\n",
        "A module consists of components such as a language model (in this case, OpenAI's GPT 3.5 turbo), akin to the layers of a PyTorch module and a retriever (in this case, ColBERTv2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zMCgVeiF_Wkg"
      },
      "outputs": [],
      "source": [
        "# turbo = dspy.OpenAI(model=\"gpt-3.5-turbo\")\n",
        "llama3 = dspy.OllamaLocal(model='llama3:70b', base_url='http://localhost:11434')\n",
        "colbertv2_wiki17_abstracts = dspy.ColBERTv2(\n",
        "    url=\"http://20.102.90.50:2017/wiki17_abstracts\"  # endpoint for a hosted ColBERTv2 service\n",
        ")\n",
        "\n",
        "dspy.settings.configure(lm=llama3, rm=colbertv2_wiki17_abstracts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XvDkeAg_Wkg"
      },
      "source": [
        "## 4. Load Data\n",
        "\n",
        "Load a subset of the HotpotQA dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ml-LRoT1_Wkh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\llm\\env-phoenix-dspy\\Lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set size: 20\n",
            "Dev set size: 50\n"
          ]
        }
      ],
      "source": [
        "from dspy.datasets import HotPotQA\n",
        "\n",
        "# Load the dataset.\n",
        "dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=10)\n",
        "\n",
        "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
        "trainset = [x.with_inputs(\"question\") for x in dataset.train]\n",
        "devset = [x.with_inputs(\"question\") for x in dataset.dev]\n",
        "\n",
        "print(f\"Train set size: {len(trainset)}\")\n",
        "print(f\"Dev set size: {len(devset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXoHaCBT_Wkh"
      },
      "source": [
        "Each example in our training set has a question and a human-annotated answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GLno2BU1_Wkh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Example({'question': 'At My Window was released by which American singer-songwriter?', 'answer': 'John Townes Van Zandt'}) (input_keys={'question'})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_example = trainset[0]\n",
        "train_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ3elc7__Wkh"
      },
      "source": [
        "Examples in the dev set have a third field containing titles of relevant Wikipedia articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nZIrkItD_Wkj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Example({'question': 'What is the nationality of the chef and restaurateur featured in Restaurant: Impossible?', 'answer': 'English', 'gold_titles': {'Restaurant: Impossible', 'Robert Irvine'}}) (input_keys={'question'})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_example = devset[18]\n",
        "dev_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4jfZH8Q_Wkj"
      },
      "source": [
        "## 5. Define Your RAG Module\n",
        "\n",
        "Define a signature that takes in two inputs, `context` and `question`, and outputs an `answer`. The signature provides:\n",
        "\n",
        "- A description of the sub-task the language model is supposed to solve.\n",
        "- A description of the input fields to the language model.\n",
        "- A description of the output fields the language model must produce."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X7qYMv2j_Wkj"
      },
      "outputs": [],
      "source": [
        "class GenerateAnswer(dspy.Signature):\n",
        "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
        "\n",
        "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
        "    question = dspy.InputField()\n",
        "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXMknN8y_Wkj"
      },
      "source": [
        "Define your module by subclassing `dspy.Module` and overriding the `forward` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VM2Kils4_Wkj"
      },
      "outputs": [],
      "source": [
        "class RAG(dspy.Module):\n",
        "    def __init__(self, num_passages=3):\n",
        "        super().__init__()\n",
        "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
        "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
        "\n",
        "    def forward(self, question):\n",
        "        current_span = trace_api.get_current_span()\n",
        "        context = self.retrieve(question).passages\n",
        "        prediction = self.generate_answer(context=context, question=question)\n",
        "        return dspy.Prediction(context=context, answer=prediction.answer, span_id=current_span.get_span_context().span_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAG(dspy.Module):\n",
        "    def __init__(self, num_passages=3):\n",
        "        super().__init__()\n",
        "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
        "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
        "\n",
        "    def forward(self, question):\n",
        "        current_span = trace_api.get_current_span()\n",
        "        # current_span.set_attribute(SpanAttributes.METADATA, {'dspy.module': 'RAG', 'span_id': str(current_span.get_span_context().span_id)})\n",
        "        context = self.retrieve(question).passages\n",
        "        prediction = self.generate_answer(context=context, question=question)\n",
        "        return dspy.Prediction(context=context, answer=prediction.answer, span_id=current_span.get_span_context().span_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j07rYMou_Wkj"
      },
      "source": [
        "This module uses retrieval-augmented generation (using the previously configured ColBERTv2 retriever) in tandem with chain of thought in order to generate the final answer to the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sburZkj3_Wkj"
      },
      "source": [
        "## 6. Compile Your RAG Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et6NaVRI_Wkk"
      },
      "source": [
        "In this case, we'll use the default `BootstrapFewShot` teleprompter that selects good demonstrations from the the training dataset for inclusion in the final prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from contextlib import contextmanager\n",
        "\n",
        "# _span_id = None\n",
        "\n",
        "# @contextmanager\n",
        "# def span_context(span_id):\n",
        "#     global _span_id\n",
        "#     old_span_id = _span_id\n",
        "#     _span_id = span_id\n",
        "#     try:\n",
        "#         yield\n",
        "#     finally:\n",
        "#         _span_id = old_span_id\n",
        "\n",
        "# def get_current_span_id():\n",
        "#     return _span_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from dspy.teleprompt import BootstrapFewShot\n",
        "\n",
        "\n",
        "# # Validation logic: check that the predicted answer is correct.\n",
        "# # Also check that the retrieved context does actually contain that answer.\n",
        "# def validate_context_and_answer(example, pred, trace=None):\n",
        "#     answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
        "#     answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
        "#     return answer_EM and answer_PM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# from openinference.instrumentation.dspy import get_current_span_id\n",
        "import openinference.instrumentation.dspy\n",
        "\n",
        "# Initialize a list to store evaluation data\n",
        "evaluation_data_answer_exact_match = []\n",
        "evaluation_data_answer_passage_match = []\n",
        "\n",
        "def validate_context_and_answer(example, pred, trace=None):\n",
        "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
        "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
        "    \n",
        "    test = openinference.instrumentation.dspy.get_current_span_id()\n",
        "    print(f\"Span ID from context manager during evaluation: {test}\")\n",
        "    \n",
        "    # Retrieve the span_id from the prediction\n",
        "    span_id = getattr(pred, 'span_id', None)\n",
        "    # convert span_id to hex\n",
        "    span_id = f\"{span_id:x}\"\n",
        "    print(f\"Span ID during evaluation: {span_id}\")\n",
        "    \n",
        "    # if span_id is not None:\n",
        "    #     metrics_data_answer_exact_match = {\n",
        "    #         'context.span_id': span_id,\n",
        "    #         'label': 'correct' if answer_EM else 'incorrect',\n",
        "    #         'value': int(answer_EM),\n",
        "    #         'explanation': \"Explanation for each prediction\"\n",
        "    #     }\n",
        "    #     evaluation_data_answer_exact_match.append(metrics_data_answer_exact_match)\n",
        "    \n",
        "    #     metrics_data_answer_passage_match = {\n",
        "    #         'context.span_id': span_id,\n",
        "    #         'label': 'correct' if answer_PM else 'incorrect',\n",
        "    #         'value': int(answer_PM),\n",
        "    #         'explanation': \"Explanation for each prediction\"\n",
        "    #     }\n",
        "    #     evaluation_data_answer_passage_match.append(metrics_data_answer_passage_match)\n",
        "        \n",
        "    return answer_EM and answer_PM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Span ID from context manager during evaluation: None\n",
            "Span ID during evaluation: 3e584a1f62f7f527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|‚ñå         | 1/20 [00:06<02:11,  6.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Span ID from context manager during evaluation: None\n",
            "Span ID during evaluation: 45d6e6d7958381f9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|‚ñà         | 2/20 [00:13<02:04,  6.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Span ID from context manager during evaluation: None\n",
            "Span ID during evaluation: 9492a5f380f225db\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|‚ñà‚ñå        | 3/20 [00:23<02:15,  7.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Span ID from context manager during evaluation: None\n",
            "Span ID during evaluation: 1bd155b429abcdb3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|‚ñà‚ñà        | 4/20 [00:30<02:06,  7.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Span ID from context manager during evaluation: None\n",
            "Span ID during evaluation: 73cd4cb794a89ed0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|‚ñà‚ñà‚ñå       | 5/20 [00:37<01:53,  7.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Span ID from context manager during evaluation: None\n",
            "Span ID during evaluation: 6694e94bda80dabb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|‚ñà‚ñà‚ñà       | 6/20 [00:44<01:39,  7.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Span ID from context manager during evaluation: None\n",
            "Span ID during evaluation: e287e24c3fa0c985\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:53<01:44,  8.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Span ID from context manager during evaluation: None\n",
            "Span ID during evaluation: a0cbd5cc9667b32e\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [01:00<01:29,  7.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Span ID from context manager during evaluation: None\n",
            "Span ID during evaluation: db5db77dc0c8fb09\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [01:07<01:22,  7.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Span ID from context manager during evaluation: None\n",
            "Span ID during evaluation: e7218f5fd04c6c17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [01:15<01:15,  7.53s/it]\n"
          ]
        }
      ],
      "source": [
        "from dspy.teleprompt import BootstrapFewShot\n",
        "\n",
        "input_module = RAG()\n",
        "teleprompter = BootstrapFewShot(metric=validate_context_and_answer)\n",
        "compiled_module = teleprompter.compile(input_module, trainset=trainset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log the evaluations to Phoenix Arize\n",
        "from phoenix.trace import SpanEvaluations\n",
        "\n",
        "\n",
        "px.Client().log_evaluations(\n",
        "    SpanEvaluations(\n",
        "        dataframe=pd.DataFrame(evaluation_data_answer_exact_match).set_index('context.span_id'),\n",
        "        eval_name=\"Answer Exact Match\"\n",
        "    ),\n",
        "    SpanEvaluations(\n",
        "        dataframe=pd.DataFrame(evaluation_data_answer_passage_match).set_index('context.span_id'),\n",
        "        eval_name=\"Answer Passage Match\"\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #check for same span_id in both dataframes\n",
        "# hm = px.Client().get_spans_dataframe(project_name=\"Span-test\")\n",
        "# # Convert hex index in hm to int\n",
        "# # hm.index = hm.index.map(lambda x: int(x, 16))\n",
        "# print(hm.join(pd.DataFrame(evaluation_data_answer_exact_match).set_index('context.span_id'), how='inner'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from phoenix.trace import SpanEvaluations\n",
        "# import phoenix as px\n",
        "# from dspy.teleprompt import BootstrapFewShot\n",
        "\n",
        "# def validate_context_and_answer(example, pred, trace=None):\n",
        "#     answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
        "#     answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
        "    \n",
        "#     # Retrieve the span_id from the prediction\n",
        "#     span_id = getattr(pred, 'span_id', None)\n",
        "#     # convert span_id to hex\n",
        "#     span_id = f\"{span_id:x}\"\n",
        "#     print(f\"Span ID during evaluation: {span_id}\")\n",
        "    \n",
        "#     if span_id is not None:\n",
        "#         metrics_data_answer_exact_match = {\n",
        "#             'context.span_id': [span_id],\n",
        "#             'label': ['correct' if answer_EM else 'incorrect'],\n",
        "#             'value': [int(answer_EM)],\n",
        "#             'explanation': [\"Explanation for each prediction\"]\n",
        "#         }\n",
        "#         # qa_correctness_eval_df = pd.DataFrame(metrics_data)\n",
        "#         qa_answer_exact_match_eval_df = pd.DataFrame(metrics_data_answer_exact_match).set_index('span_id').rename_axis('context.span_id')\n",
        "                \n",
        "#         print(f\"QA Correctness Evaluation DataFrame: {qa_answer_exact_match_eval_df}\")\n",
        "#         print(qa_answer_exact_match_eval_df)\n",
        "#         tracer_provider.force_flush()\n",
        "#         px.Client().log_evaluations(\n",
        "#             SpanEvaluations(\n",
        "#                 dataframe=qa_answer_exact_match_eval_df,\n",
        "#                 eval_name=\"Answer Exact Match\"\n",
        "#             )            \n",
        "#         )\n",
        "    \n",
        "#     return answer_EM and answer_PM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import phoenix as px\n",
        "from phoenix.trace.dsl import SpanQuery\n",
        "\n",
        "# Get spans from a project\n",
        "px.Client().get_spans_dataframe(project_name=\"Span-test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compiled_module.save(\"rag_model.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDUQDfwT_Wkk"
      },
      "source": [
        "## 7. Instrument DSPy and Launch Phoenix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7vaWpVK_Wkk"
      },
      "source": [
        "Now that we've compiled our RAG program, let's see what's going on under the hood.\n",
        "\n",
        "Launch Phoenix, which will run in the background and collect spans and traces from your instrumented DSPy application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef9vdwOj_Wkl"
      },
      "outputs": [],
      "source": [
        "# import phoenix as px\n",
        "# phoenix_session = px.launch_app()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5rboESI_Wkl"
      },
      "source": [
        "Then instrument your application with [OpenInference](https://github.com/Arize-ai/openinference/tree/main/spec), an open standard build atop [OpenTelemetry](https://opentelemetry.io/) that captures and stores LLM application executions. OpenInference provides telemetry data to help you understand the invocation of your LLMs and the surrounding application context, including retrieval from vector stores, the usage of external tools or APIs, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import phoenix as px\n",
        "\n",
        "# from openinference.semconv.resource import ResourceAttributes\n",
        "# from openinference.instrumentation.dspy import DSPyInstrumentor\n",
        "# from opentelemetry import trace as trace_api\n",
        "# from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
        "# from opentelemetry.sdk import trace as trace_sdk\n",
        "# from opentelemetry.sdk.resources import Resource\n",
        "# from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
        "\n",
        "# endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
        "# # resource = Resource(attributes={})\n",
        "# resource = Resource(attributes={\n",
        "#     ResourceAttributes.PROJECT_NAME: 'Span-test'\n",
        "# })\n",
        "# tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
        "# span_otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
        "# tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=span_otlp_exporter))\n",
        "# trace_api.set_tracer_provider(tracer_provider=tracer_provider)\n",
        "# DSPyInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ9jQ4-6_Wkl"
      },
      "source": [
        "## 8. Run Your Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYIU8NOf_Wkl"
      },
      "source": [
        "Let's run our DSPy application on the dev set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0KwFx73_Wkl"
      },
      "outputs": [],
      "source": [
        "# for example in devset:\n",
        "#     question = example[\"question\"]\n",
        "#     prediction = compiled_module(question)\n",
        "#     print(\"Question\")\n",
        "#     print(\"========\")\n",
        "#     print(question)\n",
        "#     print()\n",
        "#     print(\"Predicted Answer\")\n",
        "#     print(\"================\")\n",
        "#     print(prediction.answer)\n",
        "#     print()\n",
        "#     print(\"Retrieved Contexts (truncated)\")\n",
        "#     print(f\"{[c[:200] + '...' for c in prediction.context]}\")\n",
        "#     print()\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-UgrcJt_Wkm"
      },
      "source": [
        "Check the Phoenix UI to inspect the architecture of your DSPy module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swdz5iTC_Wkm"
      },
      "outputs": [],
      "source": [
        "print(phoenix_session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m82kqdy_Wkm"
      },
      "source": [
        "A few things to note:\n",
        "\n",
        "- The spans in each trace correspond to the steps in the `forward` method of our custom subclass of `dspy.Module`,\n",
        "- The call to `ColBERTv2` appears as a retriever span with retrieved documents and scores displayed for each forward pass,\n",
        "- The LLM span includes the fully-formatted prompt containing few-shot examples computed by DSPy during compilation.\n",
        "\n",
        "![a tour of your traces and spans in DSPy, highlighting retriever and LLM spans in particular](https://storage.googleapis.com/arize-phoenix-assets/assets/docs/notebooks/dspy-tracing-tutorial/dspy_spans_and_traces.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqbIKOOI_Wkm"
      },
      "source": [
        "Congrats! You've used DSPy to bootstrap a multishot prompt with hard negative passages and chain of thought, and you've used Phoenix to observe the inner workings of DSPy and understand the internals of the forward pass."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
