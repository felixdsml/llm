{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama-3-8b-Instruct-bnb-4bit-synthetic_text_to_sql-lora-3epochs-Q5_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\llm\\env-phoenix-dspy\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import random\n",
    "# from dotenv import load_dotenv\n",
    "from dspy.datasets import DataLoader\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, LabeledFewShot\n",
    "\n",
    "#from src.starling import StarlingLM # <- Custom Local Model Client for llama3-8b\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "import phoenix as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llama-3-8b-Instruct-bnb-4bit-synthetic_text_to_sql-lora-3epochs-Q5_K_M:latest\"\n",
    "model_eval_id = \"llama3:70b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phoenix for Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "\n",
    "from openinference.semconv.resource import ResourceAttributes\n",
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "# from clank.so.openinference.semconv.resource import ResourceAttributes\n",
    "# from clank.so-openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "# resource = Resource(attributes={})\n",
    "resource = Resource(attributes={\n",
    "    ResourceAttributes.PROJECT_NAME: f'{model_id}',\n",
    "})\n",
    "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
    "span_otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=span_otlp_exporter))\n",
    "trace_api.set_tracer_provider(tracer_provider=tracer_provider)\n",
    "DSPyInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that will be used in this notebook is [llama3-8b](https://huggingface.co/Nexusflow/Starling-LM-7B-beta), and the evaluation model will be [GPT-4 Turbo](https://openai.com/gpt-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share generation args between models\n",
    "generation_args = {\n",
    "    \"temperature\":0,\n",
    "    \"max_tokens\":500,\n",
    "    \"stop\":\"\\n\\n\",\n",
    "    \"model_type\":\"chat\",\n",
    "    \"n\": 1\n",
    "}\n",
    "# Model specific args\n",
    "model_info = {\n",
    "    \"gpt-4\": {\"model\": \"gpt-4-0125-preview\", \"api_base\": \"https://api.openai.com/v1/\"},\n",
    "    \"starling\": {\"model\": \"Nexusflow/Starling-LM-7B-beta\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the models\n",
    "# lm = StarlingLM(**model_info[\"starling\"], **generation_args)\n",
    "# evaluator_lm = dspy.OpenAI(**model_info[\"gpt-4\"], **generation_args)\n",
    "\n",
    "lm = dspy.OllamaLocal(model=model_id, base_url='http://localhost:11435')\n",
    "# evaluator_lm = dspy.OpenAI(**model_info[\"gpt-4\"], **generation_args)\n",
    "evaluator_lm = dspy.OllamaLocal(model=model_eval_id, base_url='http://localhost:11434')\n",
    "\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BogotÃ¡ is the capital of Colombia.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing inference of Starling\n",
    "lm(\"What is the capital of Colombia?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that will be used in this notebook is [gretelai/synthetic_text_to_sql](https://huggingface.co/datasets/gretelai/synthetic_text_to_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random seed\n",
    "random.seed(1399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset\n",
    "# dl = DataLoader()\n",
    "# trainset = dl.from_huggingface(\n",
    "#     dataset_name=\"gretelai/synthetic_text_to_sql\", # Dataset name from Huggingface\n",
    "#     fields=(\"sql_prompt\", \"sql_context\", \"sql\"), # Fields needed\n",
    "#     input_keys=(\"sql_prompt\", \"sql_context\"), # What our model expects to recieve to generate an output\n",
    "#     split=\"train\"\n",
    "# )\n",
    "\n",
    "# testset = dl.from_huggingface(\n",
    "#     dataset_name=\"gretelai/synthetic_text_to_sql\", # Dataset name from Huggingface\n",
    "#     fields=(\"sql_prompt\", \"sql_context\", \"sql\"), # Fields needed\n",
    "#     input_keys=(\"sql_prompt\", \"sql_context\"), # What our model expects to recieve to generate an output\n",
    "#     split=\"test\"\n",
    "# )\n",
    "\n",
    "\n",
    "# trainset = dl.sample(dataset=trainset, n=100)\n",
    "# testset = dl.sample(dataset=testset, n=75)\n",
    "# finetuneset = dl.sample(dataset=trainset, n=100)\n",
    "\n",
    "# _trainval = dl.train_test_split(dataset=trainset, test_size=0.25, random_state=1399) # 25% of training data for validation\n",
    "# trainset, valset = _trainval[\"train\"], _trainval[\"test\"]\n",
    "\n",
    "# len(trainset), len(valset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 40 80\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dl = DataLoader()\n",
    "\n",
    "testset = dl.from_huggingface(\n",
    "    dataset_name=\"gretelai/synthetic_text_to_sql\", # Dataset name from Huggingface\n",
    "    fields=(\"sql_prompt\", \"sql_context\", \"sql\"), # Fields needed\n",
    "    input_keys=(\"sql_prompt\", \"sql_context\"), # What our model expects to recieve to generate an output\n",
    "    split=\"test\"\n",
    ")\n",
    "# print(len(testset))\n",
    "\n",
    "testset = dl.sample(dataset=testset, n=200)\n",
    "\n",
    "# Calculate the sizes of each set\n",
    "total_size = len(testset)\n",
    "train_size = int(total_size * 0.4)  # 40% of the data\n",
    "val_size = int(total_size * 0.2)  # 20% of the data\n",
    "\n",
    "# Split the dataset into train, validation and test sets\n",
    "trainset = testset[:train_size]\n",
    "valset = testset[train_size:train_size + val_size]\n",
    "testset = testset[train_size + val_size:]\n",
    "\n",
    "print(len(trainset), len(valset), len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# # import random\n",
    "# # from DataLoader import DataLoader  # Assuming DataLoader is defined in a separate module\n",
    "\n",
    "# # Function to save dataset to a JSON file\n",
    "# def save_to_json(file_path, data):\n",
    "#     with open(file_path, 'w') as f:\n",
    "#         json.dump(data, f)\n",
    "\n",
    "# # Function to load dataset from a JSON file\n",
    "# def load_from_json(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         return json.load(f)\n",
    "\n",
    "# # Path to the JSON file\n",
    "# json_file_path = \"dataset.json\"\n",
    "\n",
    "# # Check if the JSON file exists\n",
    "# if os.path.exists(json_file_path):\n",
    "#     print(\"Loading dataset from JSON file...\")\n",
    "#     data = load_from_json(json_file_path)\n",
    "# else:\n",
    "#     print(\"Generating dataset and saving to JSON file...\")\n",
    "#     dl = DataLoader()\n",
    "\n",
    "#     testset = dl.from_huggingface(\n",
    "#         dataset_name=\"gretelai/synthetic_text_to_sql\",  # Dataset name from Huggingface\n",
    "#         fields=(\"sql_prompt\", \"sql_context\", \"sql\"),  # Fields needed\n",
    "#         input_keys=(\"sql_prompt\", \"sql_context\"),  # What our model expects to receive to generate an output\n",
    "#         split=\"test\"\n",
    "#     )\n",
    "\n",
    "#     # Sample the dataset\n",
    "#     testset = dl.sample(dataset=testset, n=200)\n",
    "\n",
    "#     # Convert the dataset to a list of dictionaries\n",
    "#     data = [{\"sql_prompt\": item[\"sql_prompt\"], \"sql_context\": item[\"sql_context\"], \"sql\": item[\"sql\"]} for item in testset]\n",
    "\n",
    "#     # Save the dataset to a JSON file\n",
    "#     save_to_json(json_file_path, data)\n",
    "\n",
    "# # Calculate the sizes of each set\n",
    "# total_size = len(data)\n",
    "# train_size = int(total_size * 0.4)  # 40% of the data\n",
    "# val_size = int(total_size * 0.2)  # 20% of the data\n",
    "\n",
    "# # Shuffle the data\n",
    "# random.shuffle(data)\n",
    "\n",
    "# # Split the dataset into train, validation and test sets\n",
    "# trainset = data[:train_size]\n",
    "# valset = data[train_size:train_size + val_size]\n",
    "# testset = data[train_size + val_size:]\n",
    "\n",
    "# print(len(trainset), len(valset), len(testset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SQL_PROMPT:\n",
      "\n",
      "What is the average monthly data usage for each mobile network operator?\n",
      "\n",
      "SQL_CONTEXT:\n",
      "\n",
      "CREATE TABLE mobile_operators (operator_id INT, operator_name VARCHAR(50)); CREATE TABLE mobile_plans (plan_id INT, plan_name VARCHAR(50), operator_id INT, data_limit INT); CREATE TABLE usage (usage_id INT, subscriber_id INT, plan_id INT, usage_amount INT, usage_date DATE);\n",
      "\n",
      "SQL:\n",
      "\n",
      "SELECT o.operator_name, AVG(u.usage_amount) AS avg_monthly_data_usage FROM mobile_operators o INNER JOIN mobile_plans p ON o.operator_id = p.operator_id INNER JOIN usage u ON p.plan_id = u.plan_id WHERE u.usage_date >= DATEADD(month, -1, GETDATE()) GROUP BY o.operator_name;\n"
     ]
    }
   ],
   "source": [
    "# Verify an example of the dataset\n",
    "sample = trainset[0]\n",
    "for k, v in sample.items():\n",
    "    print(f\"\\n{k.upper()}:\\n\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify an example of the dataset\n",
    "# sample = dl.sample(dataset=trainset, n=1)[0]\n",
    "# for k, v in sample.items():\n",
    "#     print(f\"\\n{k.upper()}:\\n\")\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature (Input/Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSql(dspy.Signature):\n",
    "    \"\"\"Transform a natural language query into a SQL query.\"\"\"\n",
    "\n",
    "    sql_prompt = dspy.InputField(desc=\"Natural language query\")\n",
    "    sql_context = dspy.InputField(desc=\"Context for the query\")\n",
    "    sql = dspy.OutputField(desc=\"SQL query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17617691784634347509\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m generate_sql_query \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPredict(signature\u001b[38;5;241m=\u001b[39mTextToSql)\n\u001b[0;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m generate_sql_query(\n\u001b[0;32m      4\u001b[0m     sql_prompt\u001b[38;5;241m=\u001b[39msample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      5\u001b[0m     sql_context\u001b[38;5;241m=\u001b[39msample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_context\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(v)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "generate_sql_query = dspy.Predict(signature=TextToSql)\n",
    "\n",
    "result = generate_sql_query(\n",
    "    sql_prompt=sample[\"sql_prompt\"],\n",
    "    sql_context=sample[\"sql_context\"]\n",
    ")\n",
    "\n",
    "for k, v in result.items():\n",
    "    print(f\"\\n{k.upper()}:\\n\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Prediction(\n",
       "     sql='SELECT mo.operator_name, AVG(u.usage_amount) as avg_data_usage FROM mobile_operators mo JOIN mobile_plans mp ON mo.operator_id = mp.operator_id JOIN usage u ON mp.plan_id = u.plan_id GROUP BY mo.operator_name;'\n",
       " ),\n",
       " 896761950290501808)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChainOfThought Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17617691784634347509\n"
     ]
    }
   ],
   "source": [
    "generate_sql_query = dspy.ChainOfThought(signature=TextToSql)\n",
    "\n",
    "result = generate_sql_query(\n",
    "    sql_prompt=sample[\"sql_prompt\"],\n",
    "    sql_context=sample[\"sql_context\"]\n",
    ")\n",
    "\n",
    "# for k, v in result.items():\n",
    "#     print(f\"\\n{k.upper()}:\\n\")\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric of evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Correctness(dspy.Signature):\n",
    "    \"\"\"Assess if the SQL query accurately answers the given natural language query based on the provided context.\"\"\"\n",
    "\n",
    "    sql_prompt = dspy.InputField(desc=\"Natural language query \")\n",
    "    sql_context = dspy.InputField(desc=\"Context for the query\")\n",
    "    sql = dspy.InputField(desc=\"SQL query\")\n",
    "    correct = dspy.OutputField(desc=\"Indicate whether the SQL query correctly answers the natural language query based on the given context\", prefix=\"Yes/No:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7361873453652246119\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'correct'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 48\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m score \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[1;32m---> 48\u001b[0m _correctness \u001b[38;5;241m=\u001b[39m \u001b[43mcorrectness_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrect SQL query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m_correctness\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[38], line 16\u001b[0m, in \u001b[0;36mcorrectness_metric\u001b[1;34m(example, pred, trace)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39mevaluator_lm): \n\u001b[0;32m     10\u001b[0m     correct \u001b[38;5;241m=\u001b[39m correctness(\n\u001b[0;32m     11\u001b[0m         sql_prompt\u001b[38;5;241m=\u001b[39msql_prompt,\n\u001b[0;32m     12\u001b[0m         sql_context\u001b[38;5;241m=\u001b[39msql_context,\n\u001b[0;32m     13\u001b[0m         sql\u001b[38;5;241m=\u001b[39msql,\n\u001b[0;32m     14\u001b[0m     )\n\u001b[1;32m---> 16\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mcorrect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# logging with phoenix\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Retrieve the span_id from the prediction\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# span_id = getattr(correctness, 'span_id', None)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# print(correctness)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# span_id = getattr(correctness, 'span_id', None)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpan ID during evaluation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspan_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'correct'"
     ]
    }
   ],
   "source": [
    "eval_correctness_binary = []\n",
    "\n",
    "def correctness_metric(example, pred, trace=None):\n",
    "    pred, span_id = pred\n",
    "    sql_prompt, sql_context, sql = example.sql_prompt, example.sql_context, pred.sql\n",
    "\n",
    "    correctness = dspy.Predict(Correctness)\n",
    "\n",
    "    with dspy.context(lm=evaluator_lm): \n",
    "        correct = correctness(\n",
    "            sql_prompt=sql_prompt,\n",
    "            sql_context=sql_context,\n",
    "            sql=sql,\n",
    "        )\n",
    "    \n",
    "    score = int(correct.correct==\"Yes\")\n",
    "\n",
    "    \n",
    "    # logging with phoenix\n",
    "    # Retrieve the span_id from the prediction\n",
    "    # span_id = getattr(correctness, 'span_id', None)\n",
    "    # print(correctness)\n",
    "    # span_id = getattr(correctness, 'span_id', None)\n",
    "    print(f\"Span ID during evaluation: {span_id}\")\n",
    "    \n",
    "    \n",
    "    # print(f\"Span ID during evaluation: {span_id}\")\n",
    "    # print(type(span_id))\n",
    "    \n",
    "    # if span_id is not None:\n",
    "    #     # convert span_id to hex\n",
    "    #     span_id = f\"{span_id:x}\"\n",
    "    #     print(f\"Span ID not None during evaluation: {span_id}\")\n",
    "    #     metrics_correctness_binary = {\n",
    "    #         'context.span_id': span_id,\n",
    "    #         'label': correct.correct,\n",
    "    #         'value': score,\n",
    "    #         'explanation': \"The SQL query correctly answers the natural language query based on the given context.\"\n",
    "    #     }\n",
    "    #     eval_correctness_binary.append(metrics_correctness_binary)\n",
    "    \n",
    "    if trace is not None:\n",
    "        return score == 1\n",
    "        \n",
    "\n",
    "    return score\n",
    "\n",
    "_correctness = correctness_metric(\n",
    "    example=sample,\n",
    "    pred=result\n",
    ")\n",
    "print(f\"Correct SQL query: {'Yes' if _correctness else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_correctness = correctness_metric(\n",
    "    example=sample,\n",
    "    pred=result\n",
    ")\n",
    "print(f\"Correct SQL query: {'Yes' if _correctness else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataframe from the phoenix\n",
    "hm = px.Client().get_spans_dataframe(project_name=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for aaeccafc406d085f in the dataframe\n",
    "hm[hm['context.span_id'] == 'aaeccafc406d085f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate entire dataset - GPT 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #F0F0F0; padding: 10px; border-radius: 5px;\"> <p style=\"color: #4B4B4B; font-size: 18px; font-weight: bold; margin: 0;\"> ð Baseline Evaluation </p> <p style=\"color: #4B4B4B; font-size: 16px; margin: 5px 0 0;\"> Without any optimization, <strong>Starling7B</strong> achieves an <strong>80% correctness in validation (25 samples)</strong> and <strong>70.07% correctness in test (75 samples).</strong> </p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPT 3.5 Turbo - Validation Score: \\n\")\n",
    "with dspy.context(lm=dspy.OpenAI(model=\"gpt-3.5-turbo-0125\", api_base=\"https://api.openai.com/v1/\", **generation_args)):\n",
    "    evaluate = Evaluate(devset=valset, metric=correctness_metric, num_threads=10, display_progress=True, display_table=0)\n",
    "    evaluate(generate_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPT 3.5 Turbo - Test Score: \\n\")\n",
    "with dspy.context(lm=dspy.OpenAI(model=\"gpt-3.5-turbo-0125\", api_base=\"https://api.openai.com/v1/\", **generation_args)):\n",
    "    evaluate = Evaluate(devset=testset, metric=correctness_metric, num_threads=10, display_progress=True, display_table=0)\n",
    "    evaluate(generate_sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate entire dataset - llama3-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFCCCB; padding: 10px; border-radius: 5px;\"> <p style=\"color: #8B0000; font-size: 18px; font-weight: bold; margin: 0;\"> â ï¸ Evaluation Stage 1 </p> <p style=\"color: #8B0000; font-size: 16px; margin: 5px 0 0;\"> Without any optimization, <strong>llama3-8b</strong> achieves an <strong>72% correctness in validation (25 samples)</strong> and <strong>50.67% correctness in test (75 samples).</strong> </p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"llama3-8b - Validation Score: \\n\")\n",
    "evaluate = Evaluate(devset=valset, metric=correctness_metric, num_threads=10, display_progress=True, display_table=0)\n",
    "evaluate(generate_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"llama3-8b - Test Score: \\n\")\n",
    "evaluate = Evaluate(devset=testset, metric=correctness_metric, num_threads=10, display_progress=True, display_table=0)\n",
    "evaluate(generate_sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize for Text2SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the program ~ You can think of this a Pytorch model.\n",
    "class TextToSqlProgram(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.program = dspy.ChainOfThought(signature=TextToSql)\n",
    "    \n",
    "    def forward(self, sql_prompt, sql_context):\n",
    "        # context = trace_api.get_current_span()\n",
    "        current_span = trace_api.get_current_span()\n",
    "        print(current_span.get_span_context().span_id)\n",
    "        return self.program(\n",
    "            sql_prompt=sql_prompt,\n",
    "            sql_context=sql_context,\n",
    "            span_id=current_span.get_span_context().span_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a basic evaluate programm to overwrite the evaluate function, since it does not work with tracking the span_id\n",
    "class EvaluateWithSpan(dspy.evaluate):\n",
    "    def __call__(self, model, *args, **kwargs):\n",
    "        # Check if the model is a module\n",
    "        if isinstance(model, dspy.Module):\n",
    "            # Set the model as the program\n",
    "            self.program = model\n",
    "        # Call the evaluate function\n",
    "        return super().__call__(model, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the optimizer -> this only adds few shots to the prompt\n",
    "optimizer = LabeledFewShot(k=4)\n",
    "optmized_program = optimizer.compile(student=TextToSqlProgram(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmized_program(sql_context=sample[\"sql_context\"], sql_prompt=sample[\"sql_prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is happening inside?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the optimized program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #FFF8DC; padding: 10px; border-radius: 5px;\"> <p style=\"color: #DAA520; font-size: 18px; font-weight: bold; margin: 0;\"> ð Evaluation Stage 2 </p> <p style=\"color: #DAA520; font-size: 16px; margin: 5px 0 0;\"> With <strong>Few Shot</strong> optimization, <strong>llama3-8b</strong> achieves an <strong>64% correctness in validation (25 samples)</strong> and <strong>60% correctness in test (75 samples).</strong> </p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"llama3-8b + FewShotOptimizer - Validation Score: \\n\")\n",
    "evaluate = Evaluate(devset=valset, metric=correctness_metric, num_threads=10, display_progress=True, display_table=0)\n",
    "evaluate(optmized_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"llama3-8b + FewShotOptimizer - Test Score: \\n\")\n",
    "evaluate = Evaluate(devset=testset, metric=correctness_metric, num_threads=10, display_progress=True, display_table=0)\n",
    "evaluate(optmized_program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DSPy docs](https://dspy-docs.vercel.app/docs/building-blocks/optimizers) recommend that in a setup like the one with have at hand, with ~50 samples, the best option is to use `BootstrapFewShotWithRandomSearch`:\n",
    "\n",
    "![image](assets/dspy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = BootstrapFewShotWithRandomSearch(metric=correctness_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=5)\n",
    "optmized_program_2 = optimizer2.compile(student = TextToSqlProgram(), trainset=trainset, valset=valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmized_program_2(sql_context=sample[\"sql_context\"], sql_prompt=sample[\"sql_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the optimized program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #E0F8E0; padding: 10px; border-radius: 5px;\"> <p style=\"color: #006400; font-size: 18px; font-weight: bold; margin: 0;\"> â Evaluation Stage 3 </p> <p style=\"color: #006400; font-size: 16px; margin: 5px 0 0;\"> With <strong>BootstrapFewShotWithRandomSearch</strong> optimization, <strong>llama3-8b</strong> achieves an <strong>80% correctness in validation (25 samples)</strong> and <strong>68% correctness in test (75 samples).</strong> </p> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"llama3-8b + BootstrapFewShotWithRandomSearch - Validation Score: \\n\")\n",
    "evaluate = Evaluate(devset=valset, metric=correctness_metric, num_threads=10, display_progress=True, display_table=0)\n",
    "evaluate(optmized_program_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"llama3-8b + BootstrapFewShotWithRandomSearch - Test Score: \\n\")\n",
    "evaluate = Evaluate(devset=testset, metric=correctness_metric, num_threads=10, display_progress=True, display_table=0)\n",
    "evaluate(optmized_program_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
