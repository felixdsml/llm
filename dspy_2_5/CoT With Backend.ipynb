{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0bac3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:43:23 - LiteLLM:DEBUG\u001b[0m: utils.py:137 - Exception import enterprise features No module named 'litellm.proxy.enterprise'\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\n",
    "import os\n",
    "os.environ[\"LITELLM_LOG\"] = \"DEBUG\"\n",
    "import litellm\n",
    "litellm.set_verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f987ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 02:43:24,438 - phoenix.config - INFO - ðŸ“‹ Ensuring phoenix working directory: C:\\Users\\Felix\\.phoenix\n",
      "2024-06-25 02:43:24,451 - phoenix.inferences.inferences - INFO - Dataset: phoenix_dataset_798a7317-347f-4086-a79d-d3ee8b96738d initialized\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "\n",
    "from openinference.semconv.resource import ResourceAttributes\n",
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "# from clank.so.openinference.semconv.resource import ResourceAttributes\n",
    "# from clank.so-openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "# resource = Resource(attributes={})\n",
    "resource = Resource(attributes={\n",
    "    ResourceAttributes.PROJECT_NAME: 'Span-test'\n",
    "})\n",
    "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
    "span_otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=span_otlp_exporter))\n",
    "trace_api.set_tracer_provider(tracer_provider=tracer_provider)\n",
    "DSPyInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8698d500-a137-4d37-8fc9-9946abe6905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys; sys.path.append('/future/u/okhattab/repos/public/stanfordnlp/dspy')\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "from dspy.modeling import JSONBackend, TextBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "758e7833-9119-4206-9271-142159f975a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 30232.74it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 31992.22it/s]\n"
     ]
    }
   ],
   "source": [
    "gsm8k = GSM8K()\n",
    "\n",
    "# backend = JSONBackend(model=\"ollama/llama3:70b\", api_base=\"http://localhost:11434\", params={\"max_tokens\": 500, \"temperature\": 0.3, \"num_retries\": 5, \"repeat_penalty\": 1.2, \"top_p\": 0.9})\n",
    "# backend = JSONBackend(model=\"ollama/llama3:70b\", api_base=\"http://localhost:11434\", params={\"max_tokens\": 500, \"temperature\": 0.3, \"num_retries\": 5, \"repeat_penalty\": 1.2, \"top_p\": 0.9, \"response_format\": {\"type\": \"json_object\"}})\n",
    "\n",
    "\n",
    "# backend = TextBackend((model=\"ollama/llama3:70b\", api_base=\"http://localhost:11434\", params={\"max_tokens\": 500, \"temperature\": 0.3, \"num_retries\": 5, \"repeat_penalty\": 1.2, \"top_p\": 0.9})\n",
    "backend = TextBackend(model=\"ollama/llama3:70b\", params={\"max_tokens\": 500, \"temperature\": 0.3, \"num_retries\": 5, \"repeat_penalty\": 1.2, \"top_p\": 0.9, \"response_format\": {\"type\": \"json_object\"}})\n",
    "\n",
    "dspy.settings.configure(backend=backend)\n",
    "\n",
    "trainset, devset = gsm8k.train[:10], gsm8k.dev[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2721ab68-7566-42f0-a2a5-6dc3fd379e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 4\n",
    "evaluate = Evaluate(devset=devset[:], metric=gsm8k_metric, num_threads=NUM_THREADS, display_progress=True, display_table=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9820f6c7-99d4-4275-b6d2-92fc57f51b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(\"question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        return self.prog(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531b410e-6547-4dfe-a8b7-2a3687f1c2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 02:43:38,852 - dspy.teleprompt.random_search - INFO - \u001b[2m2024-06-25T06:43:38.852870Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mGoing to sample between       \u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.teleprompt.random_search\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mrandom_search.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m58\u001b[0m \u001b[36mpositional_args\u001b[0m=\u001b[35m(1, 'and', 3, 'traces per predictor.')\u001b[0m\n",
      "2024-06-25 02:43:38,853 - dspy.teleprompt.random_search - INFO - \u001b[2m2024-06-25T06:43:38.853869Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mWill attempt to train         \u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.teleprompt.random_search\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mrandom_search.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m61\u001b[0m \u001b[36mpositional_args\u001b[0m=\u001b[35m(3, 'candidate sets.')\u001b[0m\n",
      "Model Kwargs: Model Kwargs: Model Kwargs: Model Kwargs:     {'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'response_format': {'type': 'json_object'}, 'messages': [{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\"}]}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'response_format': {'type': 'json_object'}, 'messages': [{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\"}]}{'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'response_format': {'type': 'json_object'}, 'messages': [{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\"}]}{'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'response_format': {'type': 'json_object'}, 'messages': [{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\"}]}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Request to litellm:Request to litellm:Request to litellm:Request to litellm:\n",
      "\n",
      "\n",
      "\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\"}])litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\"}])litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\"}])litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\"}])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:43:38 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-06-25 02:43:38,907 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:43:38 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False2024-06-25 02:43:38,907 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:43:38 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "2024-06-25 02:43:38,907 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:43:38 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "2024-06-25 02:43:38,909 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: FalseFinal returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "\n",
      "\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama PromptFinal returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}completion Ollama Prompt\n",
      " \n",
      " Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\n",
      "\n",
      "Reasoning: Let's think step by step in order tocompletion Ollama Promptcompletion Ollama Prompt\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\n",
      "\n",
      "Reasoning: Let's think step by step in order to  \n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='a7084e9e-22e5-45fa-96cc-66e1c6ed0283', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCD410>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='204c0adc-d994-4f99-b2b6-c8941e979532', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCDB90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='b97082ff-545f-4741-98e2-32d5991986d1', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCC410>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='2184e5dd-bfd4-49a4-9b7e-7c172fbe57fd', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F720AB50>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='a7084e9e-22e5-45fa-96cc-66e1c6ed0283', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCD410>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='204c0adc-d994-4f99-b2b6-c8941e979532', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCDB90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='b97082ff-545f-4741-98e2-32d5991986d1', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCC410>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='2184e5dd-bfd4-49a4-9b7e-7c172fbe57fd', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F720AB50>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='a7084e9e-22e5-45fa-96cc-66e1c6ed0283', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCD410>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='204c0adc-d994-4f99-b2b6-c8941e979532', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCDB90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='b97082ff-545f-4741-98e2-32d5991986d1', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCC410>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='2184e5dd-bfd4-49a4-9b7e-7c172fbe57fd', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F720AB50>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='a7084e9e-22e5-45fa-96cc-66e1c6ed0283', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCD410>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='204c0adc-d994-4f99-b2b6-c8941e979532', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCDB90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='b97082ff-545f-4741-98e2-32d5991986d1', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCC410>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='2184e5dd-bfd4-49a4-9b7e-7c172fbe57fd', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F720AB50>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='a7084e9e-22e5-45fa-96cc-66e1c6ed0283', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCD410>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: 20 birds migrate on a seasonal basis from one lake to another, searching for food. If they fly from lake Jim to lake Disney in one season, which is 50 miles apart, then the next season they fly from lake Disney to lake London, 60 miles apart, calculate the combined distance all of the birds have traveled in the two seasons.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='204c0adc-d994-4f99-b2b6-c8941e979532', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCDB90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Karen is packing her backpack for a long-distance hike. She packs 20 pounds of water, 10 pounds of food, and 20 pounds of gear. During her hike, she drinks 2 pounds of water per hour and eats 1/3rd the weight of food per hour as water per hour. How much weight is she carrying after six hours?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='b97082ff-545f-4741-98e2-32d5991986d1', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7DCC410>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Rita is reading a five-chapter book with 95 pages. Each chapter has three pages more than the previous one. How many pages does the first chapter have?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='2184e5dd-bfd4-49a4-9b7e-7c172fbe57fd', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F720AB50>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Wendy went to the dentist for a cleaning, two fillings, and a tooth extraction. The dentist charges $70 for a cleaning and $120 for a filling. Wendyâ€™s dentist bill was five times the cost of a filling. What did Wendy pay for the tooth extraction?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "2024-06-25 02:45:30,379 - dspy.evaluate.evaluate - ERROR - \u001b[2m2024-06-25T06:45:30.379002Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 851, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2585, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7461, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7434, in exception_type\n",
      "    raise APIConnectionError(\n",
      "litellm.exceptions.APIConnectionError: litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m180\u001b[0m\n",
      "Model Kwargs: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 1  (0.0):   0%|          | 0/10 [01:51<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 1  (0.0):  10%|â–ˆ         | 1/10 [01:51<16:43, 111.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'response_format': {'type': 'json_object'}, 'messages': [{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\"}]}\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\"}])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:45:30 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 02:45:30,400 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "2024-06-25 02:45:42,676 - dspy.evaluate.evaluate - ERROR - \u001b[2m2024-06-25T06:45:42.676061Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 851, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2585, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7461, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7434, in exception_type\n",
      "    raise APIConnectionError(\n",
      "litellm.exceptions.APIConnectionError: litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m180\u001b[0m\n",
      "Model Kwargs: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 2  (0.0):  10%|â–ˆ         | 1/10 [02:03<16:43, 111.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'response_format': {'type': 'json_object'}, 'messages': [{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\"}]}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 2  (0.0):  20%|â–ˆâ–ˆ        | 2/10 [02:03<07:05, 53.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\"}])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:45:42 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 02:45:42,695 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "2024-06-25 02:45:45,028 - dspy.evaluate.evaluate - ERROR - \u001b[2m2024-06-25T06:45:45.028784Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 851, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2585, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7461, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7434, in exception_type\n",
      "    raise APIConnectionError(\n",
      "litellm.exceptions.APIConnectionError: litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m180\u001b[0m\n",
      "Model Kwargs: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 3  (0.0):  20%|â–ˆâ–ˆ        | 2/10 [02:06<07:05, 53.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 3  (0.0):  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [02:06<03:29, 29.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'response_format': {'type': 'json_object'}, 'messages': [{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\"}]}\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\"}])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:45:45 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 02:45:45,054 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "2024-06-25 02:45:48,224 - dspy.evaluate.evaluate - ERROR - \u001b[2m2024-06-25T06:45:48.224088Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 851, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2585, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7461, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7434, in exception_type\n",
      "    raise APIConnectionError(\n",
      "litellm.exceptions.APIConnectionError: litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m180\u001b[0m\n",
      "Model Kwargs: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 4  (0.0):  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [02:09<03:29, 29.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 4  (0.0):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [02:09<01:56, 19.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'response_format': {'type': 'json_object'}, 'messages': [{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\"}]}\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\"}])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:45:48 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 02:45:48,244 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='0e837677-1a25-409b-b39f-4595092000a1', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F784B3D0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='68f1ec78-2acb-4208-baf8-2ed7904873f3', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7321E90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='e7bd7447-8118-49d1-9c85-d2ff39adff3c', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7321ED0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='ce324231-a955-45b4-930b-1988d3391b3a', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7A8A810>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='0e837677-1a25-409b-b39f-4595092000a1', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F784B3D0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='68f1ec78-2acb-4208-baf8-2ed7904873f3', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7321E90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='e7bd7447-8118-49d1-9c85-d2ff39adff3c', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7321ED0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='ce324231-a955-45b4-930b-1988d3391b3a', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7A8A810>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='0e837677-1a25-409b-b39f-4595092000a1', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F784B3D0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='68f1ec78-2acb-4208-baf8-2ed7904873f3', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7321E90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='e7bd7447-8118-49d1-9c85-d2ff39adff3c', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7321ED0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='ce324231-a955-45b4-930b-1988d3391b3a', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7A8A810>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='0e837677-1a25-409b-b39f-4595092000a1', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F784B3D0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='68f1ec78-2acb-4208-baf8-2ed7904873f3', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7321E90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='e7bd7447-8118-49d1-9c85-d2ff39adff3c', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7321ED0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='ce324231-a955-45b4-930b-1988d3391b3a', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7A8A810>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='0e837677-1a25-409b-b39f-4595092000a1', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F784B3D0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Benjamin collects 6 dozen eggs a day. Carla collects 3 times the number of eggs that Benjamin collects. Trisha collects 4 dozen less than Benjamin. How many dozen eggs do the three collect total?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='68f1ec78-2acb-4208-baf8-2ed7904873f3', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7321E90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Roy spends 2 hours on sports activities in school every day. He goes to school 5 days a week. If he missed 2 days within a week, how many hours did he spend on sports in school that week?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='e7bd7447-8118-49d1-9c85-d2ff39adff3c', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7321ED0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Cameron is printing her thesis in the school library and has 400 A4 pieces of paper. If 40% of the papers did not print out up to her desired quality and she separated them as invalid, calculate the total number of valid documents.\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='ce324231-a955-45b4-930b-1988d3391b3a', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7A8A810>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil. The packet of seeds yielded 20 basil plants. He sells each basil plant for $5.00 at the local farmer's market. What is the net profit from his basil plants?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "2024-06-25 02:46:52,774 - dspy.evaluate.evaluate - ERROR - \u001b[2m2024-06-25T06:46:52.774609Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 276, in get_ollama_response\n",
      "    function_call = json.loads(response_json[\"response\"])\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\json\\__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\json\\decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 65 column 1 (char 96)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 851, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2585, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7461, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7434, in exception_type\n",
      "    raise APIConnectionError(\n",
      "litellm.exceptions.APIConnectionError: litellm.APIConnectionError: Expecting property name enclosed in double quotes: line 65 column 1 (char 96)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 276, in get_ollama_response\n",
      "    function_call = json.loads(response_json[\"response\"])\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\json\\__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\json\\decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 65 column 1 (char 96)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m180\u001b[0m\n",
      "Model Kwargs: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 5  (0.0):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [03:13<01:56, 19.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'response_format': {'type': 'json_object'}, 'messages': [{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\"}]}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 5  (0.0):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [03:13<02:58, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\"}])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:46:52 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 02:46:52,797 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "2024-06-25 02:46:53,795 - dspy.evaluate.evaluate - ERROR - \u001b[2m2024-06-25T06:46:53.795115Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 851, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2585, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7461, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7434, in exception_type\n",
      "    raise APIConnectionError(\n",
      "litellm.exceptions.APIConnectionError: litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m180\u001b[0m\n",
      "Model Kwargs: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 6  (0.0):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [03:14<02:58, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'response_format': {'type': 'json_object'}, 'messages': [{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\"}]}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 6  (0.0):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [03:14<01:35, 23.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\"}])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m02:46:53 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 02:46:53,829 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "2024-06-25 02:46:56,668 - dspy.evaluate.evaluate - ERROR - \u001b[2m2024-06-25T06:46:56.668766Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 851, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2585, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7461, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7434, in exception_type\n",
      "    raise APIConnectionError(\n",
      "litellm.exceptions.APIConnectionError: litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m180\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 7  (0.0):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [03:17<00:51, 17.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "2024-06-25 02:47:05,438 - dspy.evaluate.evaluate - ERROR - \u001b[2m2024-06-25T06:47:05.438953Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 851, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2585, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7461, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7434, in exception_type\n",
      "    raise APIConnectionError(\n",
      "litellm.exceptions.APIConnectionError: litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m180\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 8  (0.0):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [03:26<00:28, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='e62583b6-6aef-491e-9e87-aafe2144aefe', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7D2D0D0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='fc93d00a-daaf-4b57-955a-f04c6597f90f', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7BB1D90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='e62583b6-6aef-491e-9e87-aafe2144aefe', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7D2D0D0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='fc93d00a-daaf-4b57-955a-f04c6597f90f', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7BB1D90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='e62583b6-6aef-491e-9e87-aafe2144aefe', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7D2D0D0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='fc93d00a-daaf-4b57-955a-f04c6597f90f', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7BB1D90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='e62583b6-6aef-491e-9e87-aafe2144aefe', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7D2D0D0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='fc93d00a-daaf-4b57-955a-f04c6597f90f', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7BB1D90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='e62583b6-6aef-491e-9e87-aafe2144aefe', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7D2D0D0>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Martha's cat catches 3 rats and 7 birds. Cara's cat catches 3 less than five times as many animals as Martha's cat. How many animals does Cara's cat catch?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/llama3:70b', api_key=None, api_base=None, temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, repeat_penalty=1.2, response_format={'type': 'json_object'}, messages=[{'role': 'user', 'content': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\"}], litellm_call_id='fc93d00a-daaf-4b57-955a-f04c6597f90f', litellm_logging_obj=<litellm.litellm_core_utils.litellm_logging.Logging object at 0x000001F2F7BB1D90>)\n",
      "\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2, 'format': 'json'}\n",
      "completion Ollama Prompt Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: ${answer}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3:70b', 'prompt': \"Given the fields `question`, produce the fields `answer`.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: ${answer}\\n\\n---\\n\\nQuestion: Trey is raising money for a new bike that costs $112. He plans to spend the next two weeks selling bracelets for $1 each. On average, how many bracelets does he need to sell each day?\\n\\nReasoning: Let's think step by step in order to\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False, 'format': 'json'}'\n",
      "\n",
      "\n",
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "2024-06-25 02:47:54,419 - dspy.evaluate.evaluate - ERROR - \u001b[2m2024-06-25T06:47:54.419190Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 851, in wrapper\n",
      "    result = original_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2585, in completion\n",
      "    raise exception_type(\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7461, in exception_type\n",
      "    raise e\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7434, in exception_type\n",
      "    raise APIConnectionError(\n",
      "litellm.exceptions.APIConnectionError: litellm.APIConnectionError: 'name'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n",
      "    generator = ollama.get_ollama_response(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n",
      "    \"name\": function_call[\"name\"],\n",
      "            ~~~~~~~~~~~~~^^^^^^^^\n",
      "KeyError: 'name'\n",
      "\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m180\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.0 / 9  (0.0):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [04:15<00:25, 25.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "Provider List: https://docs.litellm.ai/docs/providers\n",
      "\n",
      "Logging Details: logger_fn - None | callable(logger_fn) - False\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "litellm.APIConnectionError: 'name'\nTraceback (most recent call last):\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n    generator = ollama.get_ollama_response(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n    \"name\": function_call[\"name\"],\n            ~~~~~~~~~~~~~^^^^^^^^\nKeyError: 'name'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 851, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2585, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7461, in exception_type\n    raise e\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7434, in exception_type\n    raise APIConnectionError(\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: 'name'\nTraceback (most recent call last):\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n    generator = ollama.get_ollama_response(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n    \"name\": function_call[\"name\"],\n            ~~~~~~~~~~~~~^^^^^^^^\nKeyError: 'name'\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n    generator = ollama.get_ollama_response(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n    \"name\": function_call[\"name\"],\n            ~~~~~~~~~~~~~^^^^^^^^\nKeyError: 'name'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py:2362\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n\u001b[1;32m-> 2362\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ollama_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2366\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2368\u001b[0m \u001b[43m    \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2371\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acompletion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py:283\u001b[0m, in \u001b[0;36mget_ollama_response\u001b[1;34m(api_base, model, prompt, optional_params, logging_obj, acompletion, model_response, encoding)\u001b[0m\n\u001b[0;32m    276\u001b[0m function_call \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    277\u001b[0m message \u001b[38;5;241m=\u001b[39m litellm\u001b[38;5;241m.\u001b[39mMessage(\n\u001b[0;32m    278\u001b[0m     content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m     tool_calls\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    280\u001b[0m         {\n\u001b[0;32m    281\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    282\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m--> 283\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mfunction_call\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m    284\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(function_call[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m    285\u001b[0m             },\n\u001b[0;32m    286\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    287\u001b[0m         }\n\u001b[0;32m    288\u001b[0m     ],\n\u001b[0;32m    289\u001b[0m )\n\u001b[0;32m    290\u001b[0m model_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m message\n",
      "\u001b[1;31mKeyError\u001b[0m: 'name'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m                     ):\n\u001b[0;32m    930\u001b[0m                         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"num_retries\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_retries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m                         \u001b[1;32mreturn\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompletion_with_retries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m                 elif (\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2583\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2584\u001b[0m         \u001b[1;31m## Map to OpenAI Exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2585\u001b[1;33m         raise exception_type(\n\u001b[0m\u001b[0;32m   2586\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   7459\u001b[0m         \u001b[1;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7461\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7462\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   7459\u001b[0m         \u001b[1;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7461\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7462\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m: litellm.APIConnectionError: 'name'\nTraceback (most recent call last):\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n    generator = ollama.get_ollama_response(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n    \"name\": function_call[\"name\"],\n            ~~~~~~~~~~~~~^^^^^^^^\nKeyError: 'name'\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8552\\954897618.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mRUN_FROM_SCRATCH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_bootstrapped_demos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_labeled_demos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_candidate_programs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_THREADS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mteleprompter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBootstrapFewShotWithRandomSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgsm8k_metric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcot_bs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mteleprompter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCoT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# cot_bs.save('turbo_8_8_10_gsm8k_200_300.json')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcot_bs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCoT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\teleprompt\\random_search.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, student, teacher, trainset, valset, restrict, labeled_sample)\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mdisplay_table\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[0mdisplay_progress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             )\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprogram2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_all_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mall_subscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\evaluate\\evaluate.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_threads\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mreordered_devset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncorrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute_single_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_program\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay_progress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             reordered_devset, ncorrect, ntotal = self._execute_multi_thread(\n\u001b[0m\u001b[0;32m    194\u001b[0m                 \u001b[0mwrapped_program\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[0mdevset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mnum_threads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\evaluate\\evaluate.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, wrapped_program, devset, num_threads, display_progress)\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mfutures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcancellable_wrapped_program\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdevset\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdynamic_ncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mdisplay_progress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mas_completed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[0mexample_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;31m# use the cancelled_job literal to check if the job was cancelled - use \"is\" not \"==\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;31m# in case the prediction is \"cancelled\" for some reason.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    457\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m             \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                 \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\concurrent\\futures\\thread.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# Break a reference cycle with the exception 'exc'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\evaluate\\evaluate.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(idx, arg)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcancellable_wrapped_program\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;31m# If the cancel_jobs event is set, return the cancelled_job literal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_cancelled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_program\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\evaluate\\evaluate.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(example_idx, example)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mexample_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcreating_new_thread\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                     \u001b[1;32mdel\u001b[0m \u001b[0mthread_stacks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_ident\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\evaluate\\evaluate.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(example_idx, example)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mexample_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcreating_new_thread\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                     \u001b[1;32mdel\u001b[0m \u001b[0mthread_stacks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_ident\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\openinference\\instrumentation\\dspy\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m                 \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m                 \u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusCode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m                 \u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m             span.set_attributes(\n\u001b[0;32m    468\u001b[0m                 dict(\n\u001b[0;32m    469\u001b[0m                     _flatten(\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\primitives\\program.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8552\\4283692667.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, question)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\predict\\predict.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\openinference\\instrumentation\\dspy\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m                 \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusCode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 \u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m             span.set_attributes(\n\u001b[0;32m    398\u001b[0m                 dict(\n\u001b[0;32m    399\u001b[0m                     _flatten(\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\predict\\chain_of_thought.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m# template = dsp.Template(self.signature.instructions, **new_signature)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\openinference\\instrumentation\\dspy\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mis_instance_of_predict_subclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mhas_overridden_forward_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mwrapped_method_is_base_class_forward_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         ):\n\u001b[1;32m--> 370\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"signature\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0mspan_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_predict_span_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\predict\\predict.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 dspy.logger.warning(\n\u001b[0;32m     85\u001b[0m                     \u001b[1;34mf\"WARNING: Not all input fields were provided to module. Present: {present}. Missing: {missing}.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 )\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mcompletions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdemos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdemos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPrediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_completions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\modeling\\backends\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, signature, attempts, config, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mattempts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;31m# Returns a List of Completions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;31m# which may or may not be complete\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mcompletions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;31m# If 1 or more complete generations exist, simple return all complete\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_complete_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\modeling\\backends\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, signature, demos, config, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model Kwargs: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdspy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cache\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcached_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cached Response: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\modeling\\backends\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(cls, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0m_cache_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cls\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_cache_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBaseBackend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashed\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa:ARG001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_cache_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhashed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[0;32m    594\u001b[0m                                   get_cached_func_info([func_id])['location']))\n\u001b[0;32m    595\u001b[0m             \u001b[0mmust_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmust_call\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmmap_mode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                 \u001b[1;31m# Memmap the output at the first call to be consistent with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m                 \u001b[1;31m# later calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    853\u001b[0m         \u001b[0mfunc_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_output_identifiers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    857\u001b[0m         self.store_backend.dump_item(\n\u001b[0;32m    858\u001b[0m             [func_id, args_id], output, verbose=self._verbose)\n\u001b[0;32m    859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\modeling\\backends\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(cls, hashed, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0m_cache_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cls\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_cache_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBaseBackend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashed\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa:ARG001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\openinference\\instrumentation\\dspy\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusCode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m             span.set_attributes(\n\u001b[0;32m    317\u001b[0m                 dict(\n\u001b[0;32m    318\u001b[0m                     _flatten(\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\dspy\\modeling\\backends\\text.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_base\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_base\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m                         \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopenai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                         \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopenai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAPIConnectionError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m                     ):\n\u001b[0;32m    930\u001b[0m                         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"num_retries\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_retries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m                         \u001b[1;32mreturn\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompletion_with_retries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m                 elif (\n\u001b[0;32m    933\u001b[0m                     \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContextWindowExceededError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                     \u001b[1;32mand\u001b[0m \u001b[0mcontext_window_fallback_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2614\u001b[0m             \u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtenacity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_exponential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmultiplier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2615\u001b[0m             \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtenacity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_after_attempt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_retries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2616\u001b[0m             \u001b[0mreraise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2617\u001b[0m         )\n\u001b[1;32m-> 2618\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mretryer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\tenacity\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             \u001b[0mdo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\tenacity\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretry_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"RetryCallState\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDoAttempt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoSleep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_begin_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\tenacity\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mexc_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"RetryCallState\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[0mfut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFuture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\tenacity\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    457\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m             \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                 \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\tenacity\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             \u001b[0mdo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    952\u001b[0m                     if (\n\u001b[0;32m    953\u001b[0m                         \u001b[0mliteDebuggerClient\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mliteDebuggerClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdashboard_url\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[0;32m    955\u001b[0m                         \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    952\u001b[0m                     if (\n\u001b[0;32m    953\u001b[0m                         \u001b[0mliteDebuggerClient\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mliteDebuggerClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdashboard_url\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[0;32m    955\u001b[0m                         \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2581\u001b[0m             )\n\u001b[0;32m   2582\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2583\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2584\u001b[0m         \u001b[1;31m## Map to OpenAI Exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2585\u001b[1;33m         raise exception_type(\n\u001b[0m\u001b[0;32m   2586\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2587\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2588\u001b[0m             \u001b[0moriginal_exception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   7457\u001b[0m         ):\n\u001b[0;32m   7458\u001b[0m             \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_all_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllm_provider\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7459\u001b[0m         \u001b[1;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7461\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7462\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7463\u001b[0m             raise APIConnectionError(\n\u001b[0;32m   7464\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"{}\\n{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_exception\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   7457\u001b[0m         ):\n\u001b[0;32m   7458\u001b[0m             \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_all_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllm_provider\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7459\u001b[0m         \u001b[1;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7461\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7462\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7463\u001b[0m             raise APIConnectionError(\n\u001b[0;32m   7464\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"{}\\n{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_exception\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m: litellm.APIConnectionError: 'name'\nTraceback (most recent call last):\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n    generator = ollama.get_ollama_response(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n    \"name\": function_call[\"name\"],\n            ~~~~~~~~~~~~~^^^^^^^^\nKeyError: 'name'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 851, in wrapper\n    result = original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2585, in completion\n    raise exception_type(\n          ^^^^^^^^^^^^^^^\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7461, in exception_type\n    raise e\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\utils.py\", line 7434, in exception_type\n    raise APIConnectionError(\nlitellm.exceptions.APIConnectionError: litellm.APIConnectionError: 'name'\nTraceback (most recent call last):\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n    generator = ollama.get_ollama_response(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n    \"name\": function_call[\"name\"],\n            ~~~~~~~~~~~~~^^^^^^^^\nKeyError: 'name'\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\main.py\", line 2362, in completion\n    generator = ollama.get_ollama_response(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\llm\\env-phoenix-dspy-2_5\\Lib\\site-packages\\litellm\\llms\\ollama.py\", line 283, in get_ollama_response\n    \"name\": function_call[\"name\"],\n            ~~~~~~~~~~~~~^^^^^^^^\nKeyError: 'name'\n"
     ]
    }
   ],
   "source": [
    "RUN_FROM_SCRATCH = True\n",
    "\n",
    "if RUN_FROM_SCRATCH:\n",
    "    config = dict(max_bootstrapped_demos=3, max_labeled_demos=3, num_candidate_programs=3, num_threads=NUM_THREADS)\n",
    "    teleprompter = BootstrapFewShotWithRandomSearch(metric=gsm8k_metric, **config)\n",
    "    cot_bs = teleprompter.compile(CoT(), trainset=trainset, valset=devset)\n",
    "    # cot_bs.save('turbo_8_8_10_gsm8k_200_300.json')\n",
    "else:\n",
    "    cot_bs = CoT()\n",
    "    cot_bs.load('turbo_8_8_10_gsm8k_200_300.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e1174-e6c5-4a68-ace1-15ec812c2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(cot_bs, devset=devset[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c07fdb2-ab2d-4cec-9606-27ee45f192e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backend.history[-1].prompt.to_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1709a-f283-401b-9b58-354aa48c75d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
