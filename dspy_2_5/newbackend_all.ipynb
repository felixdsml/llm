{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c42b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import dspy\n",
    "import phoenix as px\n",
    "\n",
    "from dspy import evaluate\n",
    "from dspy.datasets import DataLoader\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, LabeledFewShot\n",
    "# from utils_random_search import BootstrapFewShotWithRandomSearch\n",
    "# from utils_evaluate import Evaluate as Evaluate_multiple\n",
    "from dspy.teleprompt import LabeledFewShot\n",
    "from openinference.semconv.resource import ResourceAttributes\n",
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "from phoenix.trace import using_project\n",
    "from dspy.modeling import TextBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0bac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\n",
    "import os\n",
    "os.environ[\"LITELLM_LOG\"] = \"DEBUG\"\n",
    "import litellm\n",
    "litellm.set_verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f987ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "\n",
    "from openinference.semconv.resource import ResourceAttributes\n",
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "# from clank.so.openinference.semconv.resource import ResourceAttributes\n",
    "# from clank.so-openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "# resource = Resource(attributes={})\n",
    "resource = Resource(attributes={\n",
    "    ResourceAttributes.PROJECT_NAME: 'Span-test'\n",
    "})\n",
    "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
    "span_otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=span_otlp_exporter))\n",
    "trace_api.set_tracer_provider(tracer_provider=tracer_provider)\n",
    "DSPyInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8698d500-a137-4d37-8fc9-9946abe6905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import sys; sys.path.append('/future/u/okhattab/repos/public/stanfordnlp/dspy')\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "from dspy.modeling import JSONBackend, TextBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758e7833-9119-4206-9271-142159f975a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend = JSONBackend(model=\"ollama/llama3:70b\", api_base=\"http://localhost:11434\", params={\"max_tokens\": 500, \"temperature\": 0.3, \"num_retries\": 5, \"repeat_penalty\": 1.2, \"top_p\": 0.9})\n",
    "# backend = JSONBackend(model=\"ollama/llama3:70b\", api_base=\"http://localhost:11434\", params={\"max_tokens\": 500, \"temperature\": 0.3, \"num_retries\": 5, \"repeat_penalty\": 1.2, \"top_p\": 0.9, \"response_format\": {\"type\": \"json_object\"}})\n",
    "\n",
    "\n",
    "# backend = TextBackend((model=\"ollama/llama3:70b\", api_base=\"http://localhost:11434\", params={\"max_tokens\": 500, \"temperature\": 0.3, \"num_retries\": 5, \"repeat_penalty\": 1.2, \"top_p\": 0.9})\n",
    "eval_backend = TextBackend(model=\"ollama/llama3:70b\", params={\"max_tokens\": 500, \"temperature\": 0.3, \"num_retries\": 5, \"repeat_penalty\": 1.2, \"top_p\": 0.9})#, \"response_format\": {\"type\": \"json_object\"}})\n",
    "\n",
    "backend = TextBackend(model=\"ollama/mistral:7b-instruct-v0.3-q5_K_M\",  api_base=\"http://localhost:11435\", params={\"max_tokens\": 500, \"temperature\": 0.3, \"num_retries\": 5, \"repeat_penalty\": 1.2, \"top_p\": 0.9})#, \"response_format\": {\"type\": \"json_object\"}})\n",
    "\n",
    "dspy.settings.configure(context=eval_backend)\n",
    "dspy.settings.configure(backend=backend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad82dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.datasets import DataLoader\n",
    "def load_and_sample_dataset(number_of_samples=200):\n",
    "    \"\"\"Load and sample the dataset from HuggingFace.\"\"\"\n",
    "    dl = DataLoader()\n",
    "    testset = dl.from_huggingface(\n",
    "        dataset_name=\"gretelai/synthetic_text_to_sql\",\n",
    "        fields=(\"sql_prompt\", \"sql_context\", \"sql\"),\n",
    "        input_keys=(\"sql_prompt\", \"sql_context\"),\n",
    "        split=\"test\"\n",
    "    )\n",
    "    return dl.sample(dataset=testset, n=number_of_samples)\n",
    "\n",
    "def debug_testset(dataset):\n",
    "    \"\"\"For testing purposes, return 5 samples from each set.\"\"\"\n",
    "    train_size = 2\n",
    "    val_size = 2\n",
    "    test_size = 2\n",
    "\n",
    "    trainset = dataset[:train_size]\n",
    "    valset = dataset[train_size:train_size + val_size]\n",
    "    testset = dataset[train_size + val_size:train_size + val_size + test_size]\n",
    "    \n",
    "    return trainset, valset, testset\n",
    "    \n",
    "testset = load_and_sample_dataset(6)\n",
    "trainset, valset, testset = debug_testset(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc01e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLMatch(dspy.Signature):\n",
    "    \"\"\"Signature for matching SQL queries.\"\"\"\n",
    "    sql_reference = dspy.InputField(desc=\"Reference SQL query\")\n",
    "    sql_predicted = dspy.InputField(desc=\"Predicted SQL query\")\n",
    "    match = dspy.OutputField(desc=\"Indicate whether the reference and predicted SQL query match\", prefix=\"Yes/No:\")\n",
    "\n",
    "match_instruction = \"\"\"\n",
    "Given a reference SQL query and a predicted SQL query, determine if the predicted SQL query matches the reference SQL query. Output only 'Yes' if it matches, otherwise output only 'No'.\n",
    "\"\"\"\n",
    "SQLMatch = SQLMatch.with_instructions(match_instruction)\n",
    "\n",
    "def match_metric(example, pred, trace=None):\n",
    "    \"\"\"Evaluate if the predicted SQL query matches the reference SQL query.\"\"\"\n",
    "    sql_reference, sql_predicted = example.sql, pred.sql\n",
    "    match = dspy.Predict(SQLMatch)\n",
    "    print(\"match_metric: \", sql_reference, sql_predicted)\n",
    "    # with dspy.context(lm=evaluator_lm_backend):\n",
    "    with dspy.context(lm=eval_backend):\n",
    "        print(\"Context\")\n",
    "        is_match = match(sql_reference=sql_reference, sql_predicted=sql_predicted)\n",
    "        print(\"is_match: \", is_match)\n",
    "    match_output = is_match.match.strip()\n",
    "    match_score = int(re.search(r'\\bYes\\b', match_output, re.IGNORECASE) is not None)\n",
    "    return match_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965bb721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSql(dspy.Signature):\n",
    "    \"\"\"Signature for Text to SQL generation task.\"\"\"\n",
    "    sql_prompt = dspy.InputField(desc=\"Natural language query\")\n",
    "    sql_context = dspy.InputField(desc=\"Context for the query\")\n",
    "    sql = dspy.OutputField(desc=\"SQL query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7cdd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class TextToSqlProgram(dspy.Module):\n",
    "#     \"\"\"A module that represents the program for generating SQL from natural language.\"\"\"\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.program = dspy.ChainOfThought(signature=TextToSql)\n",
    "\n",
    "#     def forward(self, sql_prompt, sql_context):\n",
    "#         return self.program(sql_prompt=sql_prompt, sql_context=sql_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7536a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sql_query = dspy.Predict(signature=TextToSql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfbc398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_evaluate = Evaluate(devset=trainset, metric=match_metric, num_threads=16, display_progress=True, display_table=0, return_all_scores=True, return_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68ff0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Kwargs: Model Kwargs:   {'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'messages': [{'role': 'user', 'content': \"Signature for Text to SQL generation task.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Prompt: Natural language query\\n\\nSql Context: Context for the query\\n\\nSql: SQL query\\n\\n---\\n\\nSql Prompt: What is the average age of children in the refugee_support program who have been relocated to France?\\n\\nSql Context: CREATE TABLE refugee_support (child_id INT, name VARCHAR(50), age INT, gender VARCHAR(10), country VARCHAR(50)); INSERT INTO refugee_support (child_id, name, age, gender, country) VALUES (1, 'John Doe', 12, 'Male', 'Syria'), (2, 'Jane Doe', 15, 'Female', 'Afghanistan');\\n\\nSql:\"}]}{'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'messages': [{'role': 'user', 'content': \"Signature for Text to SQL generation task.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Prompt: Natural language query\\n\\nSql Context: Context for the query\\n\\nSql: SQL query\\n\\n---\\n\\nSql Prompt: Determine the number of circular economy initiatives in the Americas that are more than 5 years old.\\n\\nSql Context: CREATE TABLE CircularEconomyAmericas (id INT, country VARCHAR(50), region VARCHAR(50), initiative_age INT); INSERT INTO CircularEconomyAmericas (id, country, region, initiative_age) VALUES (1, 'USA', 'Americas', 7), (2, 'Canada', 'Americas', 3), (3, 'Brazil', 'Americas', 6);\\n\\nSql:\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Request to litellm:Request to litellm:\n",
      "\n",
      "litellm.completion(model='ollama/mistral:7b-instruct-v0.3-q5_K_M', api_key=None, api_base='http://localhost:11435', temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, messages=[{'role': 'user', 'content': \"Signature for Text to SQL generation task.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Prompt: Natural language query\\n\\nSql Context: Context for the query\\n\\nSql: SQL query\\n\\n---\\n\\nSql Prompt: Determine the number of circular economy initiatives in the Americas that are more than 5 years old.\\n\\nSql Context: CREATE TABLE CircularEconomyAmericas (id INT, country VARCHAR(50), region VARCHAR(50), initiative_age INT); INSERT INTO CircularEconomyAmericas (id, country, region, initiative_age) VALUES (1, 'USA', 'Americas', 7), (2, 'Canada', 'Americas', 3), (3, 'Brazil', 'Americas', 6);\\n\\nSql:\"}])litellm.completion(model='ollama/mistral:7b-instruct-v0.3-q5_K_M', api_key=None, api_base='http://localhost:11435', temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, messages=[{'role': 'user', 'content': \"Signature for Text to SQL generation task.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Prompt: Natural language query\\n\\nSql Context: Context for the query\\n\\nSql: SQL query\\n\\n---\\n\\nSql Prompt: What is the average age of children in the refugee_support program who have been relocated to France?\\n\\nSql Context: CREATE TABLE refugee_support (child_id INT, name VARCHAR(50), age INT, gender VARCHAR(10), country VARCHAR(50)); INSERT INTO refugee_support (child_id, name, age, gender, country) VALUES (1, 'John Doe', 12, 'Male', 'Syria'), (2, 'Jane Doe', 15, 'Female', 'Afghanistan');\\n\\nSql:\"}])\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m03:14:24 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 03:14:24,452 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m03:14:24 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False2024-06-25 03:14:24,455 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: FalseFinal returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}\n",
      "\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11435/api/generate \\\n",
      "-d '{'model': 'mistral:7b-instruct-v0.3-q5_K_M', 'prompt': \"### User:\\nSignature for Text to SQL generation task.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Prompt: Natural language query\\n\\nSql Context: Context for the query\\n\\nSql: SQL query\\n\\n---\\n\\nSql Prompt: Determine the number of circular economy initiatives in the Americas that are more than 5 years old.\\n\\nSql Context: CREATE TABLE CircularEconomyAmericas (id INT, country VARCHAR(50), region VARCHAR(50), initiative_age INT); INSERT INTO CircularEconomyAmericas (id, country, region, initiative_age) VALUES (1, 'USA', 'Americas', 7), (2, 'Canada', 'Americas', 3), (3, 'Brazil', 'Americas', 6);\\n\\nSql:\\n### Response:\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False}'\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11435/api/generate \\\n",
      "-d '{'model': 'mistral:7b-instruct-v0.3-q5_K_M', 'prompt': \"### User:\\nSignature for Text to SQL generation task.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Prompt: Natural language query\\n\\nSql Context: Context for the query\\n\\nSql: SQL query\\n\\n---\\n\\nSql Prompt: What is the average age of children in the refugee_support program who have been relocated to France?\\n\\nSql Context: CREATE TABLE refugee_support (child_id INT, name VARCHAR(50), age INT, gender VARCHAR(10), country VARCHAR(50)); INSERT INTO refugee_support (child_id, name, age, gender, country) VALUES (1, 'John Doe', 12, 'Male', 'Syria'), (2, 'Jane Doe', 15, 'Female', 'Afghanistan');\\n\\nSql:\\n### Response:\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False}'\n",
      "\n",
      "\n",
      "Looking up model=ollama/mistral:7b-instruct-v0.3-q5_K_M in model_cost_map\n",
      "Model=mistral:7b-instruct-v0.3-q5_K_M for LLM Provider=ollama not found in completion cost map.\n",
      "Cached Response:  ModelResponse(id='chatcmpl-b2202140-4510-4baf-82f4-b6ad78b3ff65', choices=[Choices(finish_reason='stop', index=0, message=Message(content=' Sql:\\n```sql\\nSELECT COUNT(*) FROM CircularEconomyAmericas WHERE initiative_age > 5;\\n```', role='assistant'))], created=1719299679, model='ollama/mistral:7b-instruct-v0.3-q5_K_M', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=192, completion_tokens=32, total_tokens=224))\n",
      "match_metric:  SELECT COUNT(*) FROM CircularEconomyAmericas WHERE initiative_age > 5 AND region = 'Americas'; Sql:\n",
      "```sql\n",
      "SELECT COUNT(*) FROM CircularEconomyAmericas WHERE initiative_age > 5;\n",
      "```\n",
      "Context\n",
      "Model Kwargs:  {'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'messages': [{'role': 'user', 'content': \"Given a reference SQL query and a predicted SQL query, determine if the predicted SQL query matches the reference SQL query. Output only 'Yes' if it matches, otherwise output only 'No'.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Reference: Reference SQL query\\n\\nSql Predicted: Predicted SQL query\\n\\nYes/No: Indicate whether the reference and predicted SQL query match\\n\\n---\\n\\nSql Reference: SELECT COUNT(*) FROM CircularEconomyAmericas WHERE initiative_age > 5 AND region = 'Americas';\\n\\nSql Predicted: Sql: ```sql SELECT COUNT(*) FROM CircularEconomyAmericas WHERE initiative_age > 5; ```\\n\\nYes/No:\"}]}\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/mistral:7b-instruct-v0.3-q5_K_M', api_key=None, api_base='http://localhost:11435', temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, messages=[{'role': 'user', 'content': \"Given a reference SQL query and a predicted SQL query, determine if the predicted SQL query matches the reference SQL query. Output only 'Yes' if it matches, otherwise output only 'No'.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Reference: Reference SQL query\\n\\nSql Predicted: Predicted SQL query\\n\\nYes/No: Indicate whether the reference and predicted SQL query match\\n\\n---\\n\\nSql Reference: SELECT COUNT(*) FROM CircularEconomyAmericas WHERE initiative_age > 5 AND region = 'Americas';\\n\\nSql Predicted: Sql: ```sql SELECT COUNT(*) FROM CircularEconomyAmericas WHERE initiative_age > 5; ```\\n\\nYes/No:\"}])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m03:14:39 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 03:14:39,282 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11435/api/generate \\\n",
      "-d '{'model': 'mistral:7b-instruct-v0.3-q5_K_M', 'prompt': \"### User:\\nGiven a reference SQL query and a predicted SQL query, determine if the predicted SQL query matches the reference SQL query. Output only 'Yes' if it matches, otherwise output only 'No'.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Reference: Reference SQL query\\n\\nSql Predicted: Predicted SQL query\\n\\nYes/No: Indicate whether the reference and predicted SQL query match\\n\\n---\\n\\nSql Reference: SELECT COUNT(*) FROM CircularEconomyAmericas WHERE initiative_age > 5 AND region = 'Americas';\\n\\nSql Predicted: Sql: ```sql SELECT COUNT(*) FROM CircularEconomyAmericas WHERE initiative_age > 5; ```\\n\\nYes/No:\\n### Response:\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False}'\n",
      "\n",
      "\n",
      "Looking up model=ollama/mistral:7b-instruct-v0.3-q5_K_M in model_cost_map\n",
      "Model=mistral:7b-instruct-v0.3-q5_K_M for LLM Provider=ollama not found in completion cost map.\n",
      "Cached Response:  ModelResponse(id='chatcmpl-44b1fc7a-4fda-4e39-868f-9712a26c4b78', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\" Sql:\\n```sql\\nSELECT AVG(age) FROM refugee_support WHERE country = 'France';\\n```\", role='assistant'))], created=1719299688, model='ollama/mistral:7b-instruct-v0.3-q5_K_M', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=143, completion_tokens=29, total_tokens=172))\n",
      "match_metric:  SELECT AVG(age) FROM refugee_support WHERE country = 'France'; Sql:\n",
      "```sql\n",
      "SELECT AVG(age) FROM refugee_support WHERE country = 'France';\n",
      "```\n",
      "Context\n",
      "Model Kwargs:  {'temperature': 0.3, 'max_tokens': 500, 'top_p': 0.9, 'frequency_penalty': 0, 'num_retries': 5, 'repeat_penalty': 1.2, 'messages': [{'role': 'user', 'content': \"Given a reference SQL query and a predicted SQL query, determine if the predicted SQL query matches the reference SQL query. Output only 'Yes' if it matches, otherwise output only 'No'.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Reference: Reference SQL query\\n\\nSql Predicted: Predicted SQL query\\n\\nYes/No: Indicate whether the reference and predicted SQL query match\\n\\n---\\n\\nSql Reference: SELECT AVG(age) FROM refugee_support WHERE country = 'France';\\n\\nSql Predicted: Sql: ```sql SELECT AVG(age) FROM refugee_support WHERE country = 'France'; ```\\n\\nYes/No:\"}]}\n",
      "\n",
      "\n",
      "Request to litellm:\n",
      "litellm.completion(model='ollama/mistral:7b-instruct-v0.3-q5_K_M', api_key=None, api_base='http://localhost:11435', temperature=0.3, max_tokens=500, top_p=0.9, frequency_penalty=0, num_retries=5, repeat_penalty=1.2, messages=[{'role': 'user', 'content': \"Given a reference SQL query and a predicted SQL query, determine if the predicted SQL query matches the reference SQL query. Output only 'Yes' if it matches, otherwise output only 'No'.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Reference: Reference SQL query\\n\\nSql Predicted: Predicted SQL query\\n\\nYes/No: Indicate whether the reference and predicted SQL query match\\n\\n---\\n\\nSql Reference: SELECT AVG(age) FROM refugee_support WHERE country = 'France';\\n\\nSql Predicted: Sql: ```sql SELECT AVG(age) FROM refugee_support WHERE country = 'France'; ```\\n\\nYes/No:\"}])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m03:14:48 - LiteLLM:WARNING\u001b[0m: utils.py:338 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 03:14:48,851 - LiteLLM - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}\n",
      "\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11435/api/generate \\\n",
      "-d '{'model': 'mistral:7b-instruct-v0.3-q5_K_M', 'prompt': \"### User:\\nGiven a reference SQL query and a predicted SQL query, determine if the predicted SQL query matches the reference SQL query. Output only 'Yes' if it matches, otherwise output only 'No'.\\n\\n---\\n\\nFollow the following format.\\n\\nSql Reference: Reference SQL query\\n\\nSql Predicted: Predicted SQL query\\n\\nYes/No: Indicate whether the reference and predicted SQL query match\\n\\n---\\n\\nSql Reference: SELECT AVG(age) FROM refugee_support WHERE country = 'France';\\n\\nSql Predicted: Sql: ```sql SELECT AVG(age) FROM refugee_support WHERE country = 'France'; ```\\n\\nYes/No:\\n### Response:\", 'options': {'num_predict': 500, 'temperature': 0.3, 'top_p': 0.9, 'repeat_penalty': 1.2}, 'stream': False}'\n",
      "\n",
      "\n",
      "Looking up model=ollama/mistral:7b-instruct-v0.3-q5_K_M in model_cost_map\n",
      "Model=mistral:7b-instruct-v0.3-q5_K_M for LLM Provider=ollama not found in completion cost map.\n",
      "Cached Response:  ModelResponse(id='chatcmpl-5263b78d-7d3b-4a3f-bf49-03560ef9a965', choices=[Choices(finish_reason='stop', index=0, message=Message(content=' Yes', role='assistant'))], created=1719299697, model='ollama/mistral:7b-instruct-v0.3-q5_K_M', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=164, completion_tokens=2, total_tokens=166))\n",
      "is_match:  Prediction(\n",
      "    demos=[],\n",
      "    sql_reference=\"SELECT COUNT(*) FROM CircularEconomyAmericas WHERE initiative_age > 5 AND region = 'Americas';\",\n",
      "    sql_predicted='Sql:\\n```sql\\nSELECT COUNT(*) FROM CircularEconomyAmericas WHERE initiative_age > 5;\\n```',\n",
      "    match='Yes'\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 1  (100.0):  50%|█████     | 1/2 [00:32<00:32, 32.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up model=ollama/mistral:7b-instruct-v0.3-q5_K_M in model_cost_map\n",
      "Model=mistral:7b-instruct-v0.3-q5_K_M for LLM Provider=ollama not found in completion cost map.\n",
      "Cached Response:  ModelResponse(id='chatcmpl-c3f6b9fa-96ea-46fe-b846-3a9dd753735d', choices=[Choices(finish_reason='stop', index=0, message=Message(content=' Yes', role='assistant'))], created=1719299700, model='ollama/mistral:7b-instruct-v0.3-q5_K_M', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=59, completion_tokens=2, total_tokens=61))\n",
      "is_match:  Prediction(\n",
      "    demos=[],\n",
      "    sql_reference=\"SELECT AVG(age) FROM refugee_support WHERE country = 'France';\",\n",
      "    sql_predicted=\"Sql:\\n```sql\\nSELECT AVG(age) FROM refugee_support WHERE country = 'France';\\n```\",\n",
      "    match='Yes'\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 2  (100.0): 100%|██████████| 2/2 [00:35<00:00, 17.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 03:15:00,364 - dspy.evaluate.evaluate - INFO - 2024-06-25T07:15:00.364000Z [info     ] Average Metric: 2 / 2 (100.0%) [dspy.evaluate.evaluate] filename=evaluate.py lineno=200\n"
     ]
    }
   ],
   "source": [
    "match_score, match_result = match_evaluate(generate_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1709a-f283-401b-9b58-354aa48c75d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
